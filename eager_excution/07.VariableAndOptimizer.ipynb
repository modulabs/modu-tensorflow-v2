{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tfe.Variable(5., name='weight')\n",
    "    self.B = tfe.Variable(10., name='bias')\n",
    "  def predict(self, inputs):\n",
    "    return inputs * self.W + self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 최적화 되어야 할 loss 함수.\n",
    "def loss(model, inputs, targets):\n",
    "  error = model.predict(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  3 * x + 2 주위의 여러 점들을 포함하는 간단한 데이터셋입니다.\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 68.764\n",
      "Loss at step 000: 66.087\n",
      "Loss at step 020: 30.067\n",
      "Loss at step 040: 13.972\n",
      "Loss at step 060: 6.779\n",
      "Loss at step 080: 3.565\n",
      "Loss at step 100: 2.128\n",
      "Loss at step 120: 1.486\n",
      "Loss at step 140: 1.199\n",
      "Loss at step 160: 1.071\n",
      "Loss at step 180: 1.013\n",
      "Loss at step 200: 0.988\n",
      "Loss at step 220: 0.976\n",
      "Loss at step 240: 0.971\n",
      "Loss at step 260: 0.969\n",
      "Loss at step 280: 0.968\n",
      "Final loss: 0.967\n",
      "W = 3.0146803855895996, B = 2.0153846740722656\n"
     ]
    }
   ],
   "source": [
    "# 다음을 정의합니다:\n",
    "# 1. 모델.\n",
    "# 2. 모델 파라미터에 대한 loss 함수의 미분.\n",
    "# 3. 미분 결과에 따라 변수를 갱신할 방법.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# 학습 루프\n",
    "for i in range(300):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
